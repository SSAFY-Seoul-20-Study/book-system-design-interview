# 웹 크롤러 설계

- 웹 크롤러 용도
    - 검색 엔진 인덱싱: 가장 보편적, 웹페이지를 모아 검색 엔진을 위한 로컬 인덱스 생성
        - Googlebot
    - 웹 아카이빙: 장기 보관하기 위해 웹에서 정보를 모으는 절차
        - 미국 국회 도서관, EU 웹 아카이브
    - 웹 마이닝: 인터넷에서 유용한 지식을 도출
        - 주주총회자료, 연차보고서
    - 웹 모니터링: 인터넷에서 저작권이나 상표권이 침해되는 사례 모니터링
- 웹 크롤러 속성
    - 규모확장성
    - 안정성: 비정상적 입력이나 환경에 대응
    - 예절: 짧은 시간동안 많은 요청을 보내선 안됨
    - 확장성: 새로운 형태의 콘텐츠를 지원하기 쉬워야함

## 1단계 문제 이해 및 설계범위 확정

### 개략적 규모 추정

- 매달 10억 개의 웹페이지 다운
- QPS = 10억/30일/24시간/3600초 = 대략 400 페이지/초
- 최대 QPS = 2 * QPS = 800
- 웹페이지 크기 평균 500k
- 1개월치 저장 용량: 10억 페이지 * 500k = 500TB/월
- 5년간 보관 = 30PB

## 2단계 개략적 설계안 제시 및 동의 구하기

![image](https://github.com/SSAFY-Seoul-20-Study/book-system-design-interview/assets/77006790/f7b3b2af-2f1c-4fe8-beae-980e49774c60)

**1. 시작 URL을 미수집 URL 저장소에 저장한다.**

- **시작 URL 집합**: 웹 크롤러가 크롤링을 시작하는 출발점
    
    ⇒ 크롤러가 가능한 한 많은 링크를 탐색할 수 있도록 하는 URL을 고르는 것이 바람직
    
    - 전체 URL 공간을 작은 부분집합으로 나누는 전략
    - 주제별로 다른 시작 URL 사용하는 전략
- **미수집 URL 저장소** : 다운로드할 URL을 저장 관리하는 컴포넌트, FIFO 큐

**2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다.**

- **HTML 다운로더**: 인터넷에서 웹 페이지를 다운로드 하는 컴포넌트

**3. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL의 IP 주소를 알아내고 해당 IP 주소로 접속하여 웹 페이지를 다운받는다.**

- **도메인 이름 변환기**: URL에 대응되는 IP 주소를 알아냄

**4. 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다.**

- **콘텐츠 파서** : 문제를 일으킬 수 있는 웹페이지를 방지하기 위해 파싱과 검증 절차가 필요
    - 크롤링 서버 안에 구현하면 크롤링 과정이 느려지므로 독립된 컴포넌트로 구현

**5. 중복 콘텐츠인지 확인하는 절차를 개시한다.**

**6. 중복 콘텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지 본다.**

- **중복 컨텐츠인가?** : 웹페이지의 해시 값을 비교
- **콘텐츠 저장소**: HTML 문서를 보관하는 시스템
    - 저장할 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간등을 고려하여 저장소 구현
        - 데이터 양이 너무 많은 경우 디스크에 저장
        - 인기있는 콘텐츠는 접근 지연시간을 줄이기 위해 메모리에 저장
- 이미 저장소에 있으면 처리하지 않고 버리고, 없으면 저장소에 저장한 뒤 URL 추출기로 전달한다.

**7. URL 추출기는 해당 HTML 페이지에서 링크를 골라낸다**

- 상대경로는 전부 절대경로로 변환

**8. 골라낸 링크를 URL 필터로 전달한다.**

- **URL 필터**: 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제

**9. 필터링이 끝나고 남은 URL만 중복 URL 판별 단계로 전달한다.**

**10. 이미 처리한 URL인지 확인하기 위하여, URL 저장소에 보관된 URL인지 살핀다. 이미 저장소에 있는 URL은 버린다**

- **이미 방문한 URL**: 같은 URL을 여러번 처리하는 것을 방지하기 위해 사용
    - 블룸 필터, 해시 테이블

**11. 저장소에 없는 URL은 URL 저장소에 저장할 뿐 아니라 미수집 URL 저장소에도 전달한다.**

- **URL 저장소**: 이미 방문한 URL을 보관하는 저장소

## 3단계 상세 설계

- **미수집 URL 저장소** : 다운로드할 URL을 저장 관리하는 컴포넌트, FIFO 큐
    - 예의 : 수집 대상 서버로 짧은 시간안에 너무 많은 요청을 보내는 것을 삼가야 한다.
        
        ⇒ 동일 웹사이트에 대해서는 한 번에 한 페이지만 요청
        
    - 우선순위: 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도에 따라 우선 순위 설정
    - 신선도: 웹페이지는 수시로 추가, 삭제, 변경이 이루어지므로, 이미 다운로드한 페이지라도 주기적으로 재수집이 필요
        - 웹페이지의 변경 이력 활용
        - 우선순위를 활용하여, 중요한 페이지는 좀 더 자주 재수집
    - 지속성 저장 장치
        - 대부분의 URL은 디스크에 두지만 IO 비용을 줄이기 위해 메모리 버퍼에 큐를 둬서 버퍼에 있는 데이터를 주기적으로 디스크에 기록

  <img src="https://github.com/SSAFY-Seoul-20-Study/book-system-design-interview/assets/77006790/b1ac59b1-4422-4103-ab88-117bb563b48c" width=400 />

  - 순위 결정 장치: URL 우선순위를 정하는 컴포넌트
  - 큐: 우선순위 별로 큐가 하나씩 할당, 우선순위가 높으면 선택될 확률도 올라감
  - 큐 선택기: 임의 큐에서 처리할 URL을 꺼내는 역할, 순위가 높은 큐에서 더 자주 꺼냄
  - 큐 라우터: 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장
  - 매핑 테이블: 호스트 이름과 큐 사이의 관계를 보관하는 테이블
  - FIFO 큐: 같은 호스트에 속한 URL은 언제나 같은 큐에 보관
  - 큐 선택기: 큐들을 순회하면서 큐에서 URL을 커내서 해당 큐에서 나온 URL을 다운로드 하도록 지정된 작업 스레드에 전달
  - 작업 스레드: 전달된 URL을 다운로드 하는 작업 수행, 순차 처리, 지연시간을 둘 수 있음
- **HTML 다운로더**
    - Robots.txt(로봇 제외 프로토콜) : 웹사이트가 크롤러와 소통하는 표준적 방법, 크롤러가 수집해도 되는 페이지 목록이 있음
        - ex) https://www.google.com/robots.txt
        - 크롤러는 해당 파일에 나열된 규칙을 먼저 확인해야 한다.
        - 주기적으로 다운 받아 캐시에 보관
        - 사이트의 루트 디렉터리에 robots.txt 파일이 없다면 모든 콘텐츠를 수집할 수 있도록 간주
    - 성능 최적화
        - 분산 크롤링: 크롤링 작업을 여러 서버에 분산하고 , 각 서버는 여러 스레드를 돌려 다운로드 작업을 처리
        - 도메인 이름변환 결과 캐시
            - DNS 요청을 보내고 결과를 받는 동기적 특성 때문에 크롤러 성능의 병목 중 하나
            - DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해 놓고 주기적으로 갱신하여 성능 최적화
        - 지역성: 크롤링 작업을 수행하는 서버를 지역별로 분산 ⇒ 대상 서버와 지역적으로 가까우면 다운로드 시간이 감소
        - 짧은 타임아웃: 웹 서버의 대기 시간이 길어지면, 최대 얼마나 기다릴지를 미리 설정
    - 안전성
        - 안정 해시: 다운로더 서버들에 부하를 분산할 때 적용하여 다운로더 서버를 쉽게 추가/삭제 가능
        - 크롤링 상태 및 수집 데이터 저장: 장애가 발생해도 쉽게 복구할 수 있도록 지속성 저장장치에 기록
        - 예외처리: 에러가 발생해도 작업이 이루어지도록
        - 데이터 검증: 시스템 오류 방지
- **문제 있는 콘텐츠 감지 및 회피**
    - 중복 콘텐츠 ⇒ 해시나 체크섬 사용
    - 거미덫 : 크롤러를 무한 루프에 빠뜨리도록 설계한 웹페이지
        
        ex) [spidertrapexample.com/foo/bar/foo/bar/…](http://spidertrapexample.com/foo/bar/foo/bar/…) → 무한히 깊은 디렉토리 구조
        
        ⇒ URL 최대 길이 제한
        
        ⇒ 사람의 수작업
        
- 데이터 노이즈: 거의 가치가 없음

## 4단계 마무리

- 서버측 렌더링: 많은 웹사이트가 링크를 즉석에서 만들기 때문에, 그대로 다운받아서 파싱하면 동적으로 생성되는 링크는 발견할 수 없음 ⇒ 파싱 전 서버측 렌더링 적용
- 원치 않는 페이지 필터링: 스팸 방지 컴포넌트를 두어 품질이 조악하거나 스팸성인 페이지 제외
- 데이터베이스 다중화 및 샤딩
- 수평적 규모 확장성: 다운로드를 실행할 서버가 수백 혹은 수천 대가 필요할 수 있으므로 무상태 서버를 사용
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션
